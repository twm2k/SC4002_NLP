{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ebbf29",
   "metadata": {},
   "source": [
    "\n",
    "# NLP Assignment - Question Classification\n",
    "This notebook outlines the process for data preprocessing, training, and model creation for Question Classification tasks using pretrained Word2Vec embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be75c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim.downloader as api\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad74fe",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preprocessing\n",
    "Load and preprocess the data for question classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62db3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Question Classification Data\n",
    "train_data = pd.read_csv('../Data/TREC/Raw/train.csv')  # Replace with your path\n",
    "test_data = pd.read_csv('../Data/TREC/Raw/test.csv')  # Replace with your path\n",
    "# Preprocess Data (Tokenization, padding, label encoding)\n",
    "# Add your preprocessing steps here\n",
    "train_data['text'] = train_data['text'].str.lower()\n",
    "test_data['text'] = test_data['text'].str.lower()\n",
    "\n",
    "train_data = train_data.drop(columns=['label-fine'])\n",
    "test_data = test_data.drop(columns=['label-fine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_label = train_data['label-coarse'].unique() # returns an array([])\n",
    "coarse_label = list(coarse_label) \n",
    "selection = random.sample(coarse_label, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leeha\\AppData\\Local\\Temp\\ipykernel_10380\\603029549.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'OTHERS' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_data.loc[index, 'label-coarse'] = 'OTHERS'\n",
      "C:\\Users\\leeha\\AppData\\Local\\Temp\\ipykernel_10380\\603029549.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'OTHERS' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  test_data.loc[index, 'label-coarse'] = 'OTHERS'\n"
     ]
    }
   ],
   "source": [
    "for index, row in train_data.iterrows():\n",
    "    label_coarse = train_data.loc[index, 'label-coarse']\n",
    "    if label_coarse in selection:\n",
    "        train_data.loc[index, 'label-coarse'] = 'OTHERS'\n",
    "\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    label_coarse = test_data.loc[index, 'label-coarse']\n",
    "    if label_coarse in selection:\n",
    "        test_data.loc[index, 'label-coarse'] = 'OTHERS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OTHERS</td>\n",
       "      <td>how did serfdom develop in and then leave russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OTHERS</td>\n",
       "      <td>what films featured the character popeye doyle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OTHERS</td>\n",
       "      <td>how can i find a list of celebrities ' real na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OTHERS</td>\n",
       "      <td>what fowl grabs the spotlight after the chines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>what is the full form of .com ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>OTHERS</td>\n",
       "      <td>what 's the shape of a camel 's spine ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>OTHERS</td>\n",
       "      <td>what type of currency is used in china ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>4</td>\n",
       "      <td>what is the temperature today ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>4</td>\n",
       "      <td>what is the temperature for cooking ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>OTHERS</td>\n",
       "      <td>what currency is used in australia ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5452 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label-coarse                                               text\n",
       "0          OTHERS  how did serfdom develop in and then leave russ...\n",
       "1          OTHERS   what films featured the character popeye doyle ?\n",
       "2          OTHERS  how can i find a list of celebrities ' real na...\n",
       "3          OTHERS  what fowl grabs the spotlight after the chines...\n",
       "4               2                    what is the full form of .com ?\n",
       "...           ...                                                ...\n",
       "5447       OTHERS            what 's the shape of a camel 's spine ?\n",
       "5448       OTHERS           what type of currency is used in china ?\n",
       "5449            4                    what is the temperature today ?\n",
       "5450            4              what is the temperature for cooking ?\n",
       "5451       OTHERS               what currency is used in australia ?\n",
       "\n",
       "[5452 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(train_data, test_size = 500, shuffle = True)\n",
    "\n",
    "path = '../Data/TREC/Processed/'\n",
    "\n",
    "# Export DataFrame to a CSV file\n",
    "train_df.to_csv(f'{path}train.csv', index=False)\n",
    "dev_df.to_csv(f'{path}dev.csv', index=False)\n",
    "test_data.to_csv(f'{path}test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62e068",
   "metadata": {},
   "source": [
    "\n",
    "## Loading Pretrained Word2Vec Embeddings\n",
    "Load the pretrained Word2Vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3', '2', 'OTHERS', '4', '5'}\n"
     ]
    }
   ],
   "source": [
    "path = '../Data/TREC/Processed'\n",
    "\n",
    "training_dev_df = pd.read_csv(f'{path}/dev.csv')\n",
    "training_df = pd.read_csv(f'{path}/train.csv')\n",
    "test_df = pd.read_csv(f'{path}/test.csv')\n",
    "print(set(training_df['label-coarse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "training_df['label-coarse'] = label_encoder.fit_transform(training_df['label-coarse'])\n",
    "training_dev_df['label-coarse'] = label_encoder.fit_transform(training_dev_df['label-coarse'])\n",
    "test_df['label-coarse'] = label_encoder.fit_transform(test_df['label-coarse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(training_df['label-coarse'])\n",
    "y_val = to_categorical(training_dev_df['label-coarse'])\n",
    "y_test = to_categorical(test_df['label-coarse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_df['text'])\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(training_df['text'])\n",
    "X_val_seq = tokenizer.texts_to_sequences(training_dev_df['text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "X_train = pad_sequences(X_train_seq,MAX_LEN)\n",
    "X_val = pad_sequences(X_val_seq,MAX_LEN)\n",
    "X_test = pad_sequences(X_test_seq,MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4a5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Pretrained Word2Vec Model\n",
    "w2v_model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "_, embedding_dim = w2v_model.vectors.shape\n",
    "print (embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.08007812  0.10498047  0.04980469 ...  0.00366211  0.04760742\n",
      "  -0.06884766]\n",
      " [ 0.13964844 -0.00616455  0.21484375 ...  0.05712891  0.09960938\n",
      "  -0.234375  ]\n",
      " ...\n",
      " [ 0.34570312 -0.05419922 -0.11816406 ... -0.53125     0.12890625\n",
      "   0.08740234]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.06054688  0.10693359 -0.03662109 ...  0.39257812  0.07666016\n",
      "  -0.08056641]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model:\n",
    "        embedding_vec = w2v_model[word]\n",
    "        if embedding_vec is not None:\n",
    "            embedding_matrix[i] = embedding_vec\n",
    "print (embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d642026",
   "metadata": {},
   "source": [
    "\n",
    "## Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Global Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling1D  # Import the necessary layer\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                    output_dim=embedding_matrix.shape[1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_LEN,\n",
    "                    trainable=False))\n",
    "\n",
    "# Add a Global Average Pooling layer to perform the aggregation\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Add Dense layers (as in your original code)\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "78/78 [==============================] - 2s 11ms/step - loss: 1.4549 - accuracy: 0.4418 - val_loss: 1.3889 - val_accuracy: 0.3940\n",
      "Epoch 2/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 1.2741 - accuracy: 0.4479 - val_loss: 1.2713 - val_accuracy: 0.4080\n",
      "Epoch 3/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 1.1698 - accuracy: 0.5101 - val_loss: 1.1614 - val_accuracy: 0.5180\n",
      "Epoch 4/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 1.0665 - accuracy: 0.5695 - val_loss: 1.0642 - val_accuracy: 0.5600\n",
      "Epoch 5/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.9740 - accuracy: 0.6323 - val_loss: 0.9753 - val_accuracy: 0.6640\n",
      "Epoch 6/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.8976 - accuracy: 0.6751 - val_loss: 0.9049 - val_accuracy: 0.6840\n",
      "Epoch 7/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.8373 - accuracy: 0.7007 - val_loss: 0.8522 - val_accuracy: 0.7040\n",
      "Epoch 8/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.7891 - accuracy: 0.7213 - val_loss: 0.8106 - val_accuracy: 0.7220\n",
      "Epoch 9/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.7515 - accuracy: 0.7345 - val_loss: 0.7743 - val_accuracy: 0.7380\n",
      "Epoch 10/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.7198 - accuracy: 0.7484 - val_loss: 0.7479 - val_accuracy: 0.7420\n",
      "Epoch 11/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.6939 - accuracy: 0.7559 - val_loss: 0.7269 - val_accuracy: 0.7480\n",
      "Epoch 12/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.6701 - accuracy: 0.7641 - val_loss: 0.7049 - val_accuracy: 0.7560\n",
      "Epoch 13/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.6509 - accuracy: 0.7686 - val_loss: 0.6862 - val_accuracy: 0.7700\n",
      "Epoch 14/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.6331 - accuracy: 0.7758 - val_loss: 0.6704 - val_accuracy: 0.7800\n",
      "Epoch 15/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.6180 - accuracy: 0.7777 - val_loss: 0.6555 - val_accuracy: 0.7860\n",
      "Epoch 16/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.6032 - accuracy: 0.7855 - val_loss: 0.6491 - val_accuracy: 0.7900\n",
      "Epoch 17/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5900 - accuracy: 0.7882 - val_loss: 0.6369 - val_accuracy: 0.8000\n",
      "Epoch 18/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5797 - accuracy: 0.7898 - val_loss: 0.6319 - val_accuracy: 0.7880\n",
      "Epoch 19/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5679 - accuracy: 0.7981 - val_loss: 0.6098 - val_accuracy: 0.7920\n",
      "Epoch 20/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5592 - accuracy: 0.8009 - val_loss: 0.6124 - val_accuracy: 0.7980\n",
      "Epoch 21/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5489 - accuracy: 0.8039 - val_loss: 0.5961 - val_accuracy: 0.8020\n",
      "Epoch 22/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5400 - accuracy: 0.8067 - val_loss: 0.5954 - val_accuracy: 0.8020\n",
      "Epoch 23/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5324 - accuracy: 0.8074 - val_loss: 0.5854 - val_accuracy: 0.8020\n",
      "Epoch 24/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5237 - accuracy: 0.8110 - val_loss: 0.5767 - val_accuracy: 0.8040\n",
      "Epoch 25/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5161 - accuracy: 0.8122 - val_loss: 0.5723 - val_accuracy: 0.8040\n",
      "Epoch 26/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5090 - accuracy: 0.8148 - val_loss: 0.5755 - val_accuracy: 0.7980\n",
      "Epoch 27/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5020 - accuracy: 0.8199 - val_loss: 0.5584 - val_accuracy: 0.8040\n",
      "Epoch 28/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.4958 - accuracy: 0.8211 - val_loss: 0.5539 - val_accuracy: 0.8160\n",
      "Epoch 29/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4901 - accuracy: 0.8221 - val_loss: 0.5582 - val_accuracy: 0.8100\n",
      "Epoch 30/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4843 - accuracy: 0.8288 - val_loss: 0.5470 - val_accuracy: 0.8020\n",
      "Epoch 31/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4788 - accuracy: 0.8277 - val_loss: 0.5454 - val_accuracy: 0.8080\n",
      "Epoch 32/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4725 - accuracy: 0.8302 - val_loss: 0.5427 - val_accuracy: 0.8060\n",
      "Epoch 33/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4683 - accuracy: 0.8334 - val_loss: 0.5456 - val_accuracy: 0.8100\n",
      "Epoch 34/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4635 - accuracy: 0.8356 - val_loss: 0.5312 - val_accuracy: 0.8140\n",
      "Epoch 35/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4600 - accuracy: 0.8391 - val_loss: 0.5304 - val_accuracy: 0.8240\n",
      "Epoch 36/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4547 - accuracy: 0.8415 - val_loss: 0.5416 - val_accuracy: 0.8160\n",
      "Epoch 37/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4519 - accuracy: 0.8409 - val_loss: 0.5359 - val_accuracy: 0.8260\n",
      "Epoch 38/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.4478 - accuracy: 0.8431 - val_loss: 0.5261 - val_accuracy: 0.8180\n",
      "Epoch 39/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.4425 - accuracy: 0.8429 - val_loss: 0.5179 - val_accuracy: 0.8300\n",
      "Epoch 40/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4390 - accuracy: 0.8429 - val_loss: 0.5207 - val_accuracy: 0.8220\n",
      "Epoch 41/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4374 - accuracy: 0.8435 - val_loss: 0.5174 - val_accuracy: 0.8280\n",
      "Epoch 42/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.4330 - accuracy: 0.8441 - val_loss: 0.5113 - val_accuracy: 0.8240\n",
      "Epoch 43/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.4301 - accuracy: 0.8481 - val_loss: 0.5115 - val_accuracy: 0.8240\n",
      "Epoch 44/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4268 - accuracy: 0.8485 - val_loss: 0.5179 - val_accuracy: 0.8220\n",
      "Epoch 45/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.4242 - accuracy: 0.8483 - val_loss: 0.5066 - val_accuracy: 0.8300\n",
      "Epoch 46/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4212 - accuracy: 0.8496 - val_loss: 0.5066 - val_accuracy: 0.8260\n",
      "Epoch 47/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.4194 - accuracy: 0.8500 - val_loss: 0.5159 - val_accuracy: 0.8180\n",
      "Epoch 48/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4163 - accuracy: 0.8504 - val_loss: 0.5088 - val_accuracy: 0.8280\n",
      "Epoch 49/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4143 - accuracy: 0.8510 - val_loss: 0.5090 - val_accuracy: 0.8280\n",
      "Epoch 50/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4105 - accuracy: 0.8528 - val_loss: 0.4980 - val_accuracy: 0.8300\n",
      "Epoch 51/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.4078 - accuracy: 0.8536 - val_loss: 0.5093 - val_accuracy: 0.8260\n",
      "Epoch 52/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.4078 - accuracy: 0.8526 - val_loss: 0.5029 - val_accuracy: 0.8220\n",
      "Epoch 53/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.4030 - accuracy: 0.8566 - val_loss: 0.5070 - val_accuracy: 0.8300\n",
      "Epoch 54/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.4015 - accuracy: 0.8552 - val_loss: 0.4988 - val_accuracy: 0.8280\n",
      "Epoch 55/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.3996 - accuracy: 0.8572 - val_loss: 0.4962 - val_accuracy: 0.8320\n",
      "Epoch 56/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.3991 - accuracy: 0.8580 - val_loss: 0.4950 - val_accuracy: 0.8320\n",
      "Epoch 57/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.3954 - accuracy: 0.8572 - val_loss: 0.4935 - val_accuracy: 0.8340\n",
      "Epoch 58/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.3931 - accuracy: 0.8588 - val_loss: 0.5046 - val_accuracy: 0.8300\n",
      "Epoch 59/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.3934 - accuracy: 0.8599 - val_loss: 0.5060 - val_accuracy: 0.8300\n",
      "Epoch 60/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.3895 - accuracy: 0.8605 - val_loss: 0.4982 - val_accuracy: 0.8280\n",
      "Epoch 61/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.3874 - accuracy: 0.8578 - val_loss: 0.5017 - val_accuracy: 0.8280\n",
      "Epoch 62/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.3877 - accuracy: 0.8586 - val_loss: 0.5081 - val_accuracy: 0.8240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x207aac86220>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Question Classification Model\n",
    "NUM_EPOCH = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, validation_data=(X_val, y_val), callbacks= early_stopper, workers= 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4138 - accuracy: 0.8680\n",
      "Test Loss: 0.41378143429756165, Test Accuracy: 0.8679999709129333\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Global Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling1D  # Import the necessary layer\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                    output_dim=embedding_matrix.shape[1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_LEN,\n",
    "                    trainable=False))\n",
    "\n",
    "# Add a Global Max Pooling layer to perform the aggregation\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Add Dense layers (as in your original code)\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "78/78 [==============================] - 2s 12ms/step - loss: 1.3304 - accuracy: 0.4388 - val_loss: 1.2514 - val_accuracy: 0.4460\n",
      "Epoch 2/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 1.1477 - accuracy: 0.5287 - val_loss: 1.0630 - val_accuracy: 0.5700\n",
      "Epoch 3/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 1.0122 - accuracy: 0.6036 - val_loss: 0.9386 - val_accuracy: 0.6520\n",
      "Epoch 4/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.9124 - accuracy: 0.6587 - val_loss: 0.8565 - val_accuracy: 0.6880\n",
      "Epoch 5/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.8446 - accuracy: 0.6935 - val_loss: 0.7936 - val_accuracy: 0.7040\n",
      "Epoch 6/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.7957 - accuracy: 0.7102 - val_loss: 0.8032 - val_accuracy: 0.7100\n",
      "Epoch 7/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.7483 - accuracy: 0.7274 - val_loss: 0.7399 - val_accuracy: 0.7060\n",
      "Epoch 8/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.7231 - accuracy: 0.7286 - val_loss: 0.7502 - val_accuracy: 0.7140\n",
      "Epoch 9/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.6961 - accuracy: 0.7351 - val_loss: 0.6808 - val_accuracy: 0.7440\n",
      "Epoch 10/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.6743 - accuracy: 0.7433 - val_loss: 0.6778 - val_accuracy: 0.7400\n",
      "Epoch 11/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.6655 - accuracy: 0.7413 - val_loss: 0.7006 - val_accuracy: 0.7320\n",
      "Epoch 12/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.6380 - accuracy: 0.7540 - val_loss: 0.6481 - val_accuracy: 0.7540\n",
      "Epoch 13/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.6295 - accuracy: 0.7599 - val_loss: 0.6435 - val_accuracy: 0.7640\n",
      "Epoch 14/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.6188 - accuracy: 0.7682 - val_loss: 0.6584 - val_accuracy: 0.7400\n",
      "Epoch 15/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.6047 - accuracy: 0.7712 - val_loss: 0.6475 - val_accuracy: 0.7660\n",
      "Epoch 16/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.5989 - accuracy: 0.7710 - val_loss: 0.6407 - val_accuracy: 0.7520\n",
      "Epoch 17/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.5954 - accuracy: 0.7718 - val_loss: 0.6177 - val_accuracy: 0.7640\n",
      "Epoch 18/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.5833 - accuracy: 0.7771 - val_loss: 0.6243 - val_accuracy: 0.7760\n",
      "Epoch 19/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5773 - accuracy: 0.7785 - val_loss: 0.6206 - val_accuracy: 0.7720\n",
      "Epoch 20/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.5783 - accuracy: 0.7740 - val_loss: 0.6137 - val_accuracy: 0.7820\n",
      "Epoch 21/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5690 - accuracy: 0.7809 - val_loss: 0.6384 - val_accuracy: 0.7580\n",
      "Epoch 22/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5598 - accuracy: 0.7872 - val_loss: 0.6339 - val_accuracy: 0.7640\n",
      "Epoch 23/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5549 - accuracy: 0.7872 - val_loss: 0.6279 - val_accuracy: 0.7740\n",
      "Epoch 24/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5557 - accuracy: 0.7866 - val_loss: 0.6144 - val_accuracy: 0.7760\n",
      "Epoch 25/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5529 - accuracy: 0.7863 - val_loss: 0.6115 - val_accuracy: 0.7820\n",
      "Epoch 26/200\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.5435 - accuracy: 0.7942 - val_loss: 0.6112 - val_accuracy: 0.7800\n",
      "Epoch 27/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5418 - accuracy: 0.7954 - val_loss: 0.6233 - val_accuracy: 0.7800\n",
      "Epoch 28/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5407 - accuracy: 0.7954 - val_loss: 0.6229 - val_accuracy: 0.7860\n",
      "Epoch 29/200\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.5525 - accuracy: 0.7922 - val_loss: 0.6442 - val_accuracy: 0.7520\n",
      "Epoch 30/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5379 - accuracy: 0.7914 - val_loss: 0.6538 - val_accuracy: 0.7420\n",
      "Epoch 31/200\n",
      "78/78 [==============================] - 1s 6ms/step - loss: 0.5261 - accuracy: 0.7985 - val_loss: 0.6231 - val_accuracy: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x207a9c52070>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Question Classification Model\n",
    "NUM_EPOCH = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, validation_data=(X_val, y_val), callbacks= early_stopper, workers= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5106 - accuracy: 0.8220\n",
      "Test Loss: 0.5106096267700195, Test Accuracy: 0.8220000267028809\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LSTM/RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38fa26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Question Classification\n",
    "# Define question classification model architecture\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                    output_dim=embedding_matrix.shape[1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_LEN,  # Length of input sequences\n",
    "                    trainable=False))  # Set to True if you want to fine-tune embeddings\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "# Add Dense layer(s)\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=5, activation='softmax'))  # num_classes: number of output classes\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086b120",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "Training procedures for both NER and question classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f263f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "78/78 [==============================] - 15s 137ms/step - loss: 1.0949 - accuracy: 0.5743 - val_loss: 0.7858 - val_accuracy: 0.7440\n",
      "Epoch 2/200\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 0.5162 - accuracy: 0.8314 - val_loss: 0.4206 - val_accuracy: 0.8760\n",
      "Epoch 3/200\n",
      "78/78 [==============================] - 11s 137ms/step - loss: 0.3582 - accuracy: 0.8716 - val_loss: 0.3777 - val_accuracy: 0.8840\n",
      "Epoch 4/200\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 0.2793 - accuracy: 0.9093 - val_loss: 0.3239 - val_accuracy: 0.9000\n",
      "Epoch 5/200\n",
      "78/78 [==============================] - 10s 128ms/step - loss: 0.2382 - accuracy: 0.9245 - val_loss: 0.3342 - val_accuracy: 0.8960\n",
      "Epoch 6/200\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 0.2069 - accuracy: 0.9281 - val_loss: 0.3322 - val_accuracy: 0.9000\n",
      "Epoch 7/200\n",
      "78/78 [==============================] - 10s 135ms/step - loss: 0.1806 - accuracy: 0.9344 - val_loss: 0.2964 - val_accuracy: 0.9140\n",
      "Epoch 8/200\n",
      "78/78 [==============================] - 10s 134ms/step - loss: 0.1471 - accuracy: 0.9485 - val_loss: 0.2806 - val_accuracy: 0.9060\n",
      "Epoch 9/200\n",
      "78/78 [==============================] - 10s 131ms/step - loss: 0.1369 - accuracy: 0.9511 - val_loss: 0.3359 - val_accuracy: 0.9020\n",
      "Epoch 10/200\n",
      "78/78 [==============================] - 10s 130ms/step - loss: 0.1105 - accuracy: 0.9620 - val_loss: 0.3011 - val_accuracy: 0.9200\n",
      "Epoch 11/200\n",
      "78/78 [==============================] - 10s 132ms/step - loss: 0.0939 - accuracy: 0.9665 - val_loss: 0.2881 - val_accuracy: 0.9180\n",
      "Epoch 12/200\n",
      "78/78 [==============================] - 10s 131ms/step - loss: 0.0781 - accuracy: 0.9731 - val_loss: 0.3405 - val_accuracy: 0.9260\n",
      "Epoch 13/200\n",
      "78/78 [==============================] - 10s 132ms/step - loss: 0.0602 - accuracy: 0.9796 - val_loss: 0.3490 - val_accuracy: 0.9260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x207a92b2760>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Question Classification Model\n",
    "NUM_EPOCH = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, validation_data=(X_val, y_val), callbacks= early_stopper, workers= 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965e4cb",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation\n",
    "Metrics and evaluation methods for the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ade3defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 38ms/step - loss: 0.2971 - accuracy: 0.9260\n",
      "Test Loss: 0.2971298098564148, Test Accuracy: 0.9259999990463257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate NER Model\n",
    "# Add your evaluation code here\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "# Evaluate Question Classification Model\n",
    "# Add your evaluation code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
